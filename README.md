This repository contains the solutions for the MLA202 practical test, which focused on three distinct and fundamental challenges within reinforcement learning. The first challenge was to implement a Q-learning agent to successfully navigate a custom 5x5 GridWorld environment that included obstacles or walls. The second challenge required the completion of a partially implemented Deep Q-Network (DQN) agent designed to solve the classic CartPole-v1 control problem from the OpenAI Gym suite. The third and final challenge was a debugging exercise, where the task was to identify, analyze, and correct four separate bugs present in a provided Q-learning implementation for the FrozenLake environment.

The technical approaches for each problem were carefully selected and applied. For Problem 1, which involved the GridWorld, the solution utilized tabular Q-learning, a foundational model-free reinforcement learning algorithm. This method relies on directly updating a Q-table that stores the estimated value of taking each action in every state. The implementation specifically employed an epsilon-greedy policy to balance exploration and exploitation during training. The core of the learning process was the Q-value update rule, which calculates a new value based on the immediate reward received and the discounted estimated value of the best future action. Critical hyperparameters such as the learning rate, which controls the update step size, the discount factor, which determines the importance of future rewards, and the exploration rate, were tuned to optimize the agent's performance.

For Problem 2, concerning the CartPole-v1 environment, the solution transitioned from a simple tabular method to a more advanced function approximation technique using deep learning. This involved implementing a complete Deep Q-Network, where a neural network is trained to approximate the optimal Q-value function. Key components necessary for stable and effective deep reinforcement learning were integrated, including an experience replay buffer to break the correlation between consecutive learning updates, a separate target network to provide stable Q-targets during training, and a decaying epsilon-greedy strategy for exploration. The neural network itself was optimized using the Adam optimizer and trained by minimizing the Mean Squared Error loss between the predicted Q-values and the target Q-values generated by the DQN algorithm.

Problem 3 was an exercise in comparative analysis and debugging. The technique involved a meticulous side-by-side examination of a buggy Q-learning implementation against a corrected version. The debugging process required analyzing the learning curves, success rates, and overall convergence behavior of both implementations. By plotting and comparing these metrics, the specific logical and algorithmic errors in the original code could be isolated, understood, and subsequently fixed to restore proper learning functionality.

The successful execution of these solutions generates specific graphical outputs that demonstrate the learning progress and results. The expected output files are `problem1_results.png`, which contains the training curves showing the agent's improving performance in the GridWorld; `problem2_results.png`, which displays the training progression and final performance of the DQN agent on the CartPole task; and `problem3_comparison.png`, which provides a visual juxtaposition of the learning trajectories between the buggy and the corrected FrozenLake Q-learning implementations, clearly illustrating the impact of the fixes. It is important to note that these output files are submitted separately from this text. This submission format is due to the fact that all code was executed on the Google Colab platform, as technical issues prevented the use of the local Visual Studio Code development environment. We appreciate your understanding regarding this necessary adjustment to the submission process.
