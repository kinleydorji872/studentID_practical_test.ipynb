#kinley Dorji
# 03240367
# 3RD BBI MLA
# Assingement : Home-Take assingement 
# This repository contains the complete solutions for the MLA202 practical test, which assessed proficiency across three core areas of reinforcement learning: fundamental algorithm implementation, deep learning integration, and debugging. The test comprised three distinct problems. The first problem focused on implementing Q-learning to solve a custom 5×5 GridWorld environment containing walls, challenging the agent to learn an optimal navigation policy. The second problem required the completion of a partially implemented Deep Q-Network (DQN) agent for the classic CartPole-v1 control task, bridging the gap between tabular methods and function approximation. The third problem was a debugging exercise, demanding the identification and correction of four specific bugs within a provided Q-learning implementation for the FrozenLake environment.

#The techniques employed for each problem were methodically chosen to match the task's requirements. For Problem 1, the solution utilized tabular Q-learning, a model-free reinforcement learning strategy. Key details of this implementation included an epsilon-greedy action selection policy to balance exploration and exploitation, and the foundational Q-table update rule: Q(s,a) ← Q(s,a) + α[r + γ * max_a' Q(s',a') - Q(s,a)]. Critical learning parameters such as the learning rate (α), discount factor (γ), and exploration rate (ε) were tuned to facilitate effective policy convergence in the grid-based world.

#For Problem 2, the technique shifted to Deep Q-Learning, which leverages a neural network to approximate Q-values for high-dimensional state spaces. The implementation details encompassed several vital components for stable training: a neural network that predicts Q-values for every possible action given a state, an experience replay buffer to decorrelate sequential experiences and enable steady learning, an ε-greedy exploration policy, the Adam optimizer in conjunction with a Mean Squared Error loss function to train the network, and a separate target network, updated periodically, to provide stable target Q-values and prevent harmful feedback loops during training.

#Problem 3 involved a diagnostic and comparative technique centered on Q-learning performance analysis. The debugging process entailed a detailed examination of the differences between the initial buggy implementation and the corrected version. This analysis focused on key performance metrics such as the agent's success rate and learning convergence over episodes. A plot-based visual comparison was essential to qualitatively and quantitatively demonstrate the impact of the fixes, clearly showing the restored learning capability in the corrected agent.

#The successful execution of these solutions generates the following expected output files, which visually document the results and learning progress: `problem1_results.png`, which displays the training curves for the GridWorld Q-learning agent; `problem2_results.png`, which illustrates the training progression and performance of the CartPole DQN agent; and `problem3_comparison.png`, which provides a side-by-side visual comparison of the buggy versus fixed Q-learning implementations for FrozenLake. Please note that these output files are attached separately to this submission. The reason for providing the outputs as separate files is that all code was executed on the Google Colab platform due to technical issues encountered with the local Visual Studio Code development environment. Thank you for your understanding.